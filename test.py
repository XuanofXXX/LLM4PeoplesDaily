import torch
from torch.utils.data import Dataset, DataLoader
import numpy as np
import time

# 1. åˆ›å»ºä¸€ä¸ªå¤§å‹ NumPy æ•°æ®é›†
data_size = 1000000  # æ•°æ®æ ·æœ¬æ•°é‡
feature_dim = 2560     # æ¯ä¸ªæ ·æœ¬çš„ç‰¹å¾ç»´åº¦
print(f"å‡†å¤‡ NumPy æ•°æ®é›†ï¼Œå¤§å°: ({data_size}, {feature_dim})...")
# åˆ›å»ºä¸€ä¸ªéšæœºçš„ NumPy æ•°ç»„ä½œä¸ºæˆ‘ä»¬çš„æ•°æ®é›†
# ä½¿ç”¨ float32 ä»¥æ¨¡æ‹Ÿå¸¸è§æ·±åº¦å­¦ä¹ åœºæ™¯ä¸­çš„æ•°æ®ç±»å‹
numpy_data = np.random.randn(data_size, feature_dim).astype(np.float32)
numpy_labels = np.random.randint(0, 10, size=data_size).astype(np.int64) # å‡è®¾æ˜¯10åˆ†ç±»ä»»åŠ¡
print("NumPy æ•°æ®é›†å‡†å¤‡å®Œæ¯•ã€‚\n")

# 2. å®ç°ä¸€ä¸ªè‡ªå®šä¹‰ PyTorch Dataset
class MyCustomDataset(Dataset):
    def __init__(self, data, labels):
        self.data = data
        self.labels = labels

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç›´æ¥è¿”å› NumPy æ•°ç»„ï¼ŒDataLoader ä¼šè‡ªåŠ¨å°†å…¶è½¬æ¢ä¸º Tensor
        # å¦‚æœéœ€è¦ï¼Œä¹Ÿå¯ä»¥åœ¨æ­¤å¤„è½¬æ¢ä¸º Tensor:
        # return torch.from_numpy(self.data[idx]), torch.tensor(self.labels[idx], dtype=torch.long)
        return self.data[idx], self.labels[idx]

print("åˆ›å»ºè‡ªå®šä¹‰ Dataset å®ä¾‹...")
custom_dataset = MyCustomDataset(numpy_data, numpy_labels)
print("Dataset å®ä¾‹åˆ›å»ºå®Œæ¯•ã€‚\n")

# --- éªŒè¯å¼€å§‹ ---

# 3. ç›´æ¥éå† Dataset
print("å¼€å§‹ç›´æ¥éå† Dataset...")
start_time_dataset = time.time()
count_dataset = 0
for data, label in custom_dataset:
    # æ¨¡æ‹Ÿä¸€äº›éå¸¸è½»å¾®çš„æ“ä½œï¼Œå®é™…åº”ç”¨ä¸­å¯èƒ½ä¼šæœ‰æ›´å¤æ‚çš„æ“ä½œ
    # è¿™é‡Œæˆ‘ä»¬åªæ˜¯ç®€å•åœ°è®¿é—®æ•°æ®ï¼Œä»¥æµ‹é‡çº¯ç²¹çš„è¿­ä»£å¼€é”€
    _ = data.shape
    _ = label.item()
    count_dataset += 1
end_time_dataset = time.time()
duration_dataset = end_time_dataset - start_time_dataset
print(f"ç›´æ¥éå† Dataset å®Œæˆã€‚")
print(f"éå†æ ·æœ¬æ•°: {count_dataset}")
print(f"è€—æ—¶: {duration_dataset:.4f} ç§’\n")

# 4. é€šè¿‡ DataLoader éå† Dataset (å•è¿›ç¨‹)
batch_size = 64
print(f"åˆ›å»º DataLoader å®ä¾‹ (batch_size={batch_size}, num_workers=0)...")
# num_workers=0 è¡¨ç¤ºåœ¨ä¸»è¿›ç¨‹ä¸­åŠ è½½æ•°æ®
dataloader_single_worker = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False, num_workers=0)
print("DataLoader (å•è¿›ç¨‹) å®ä¾‹åˆ›å»ºå®Œæ¯•ã€‚\n")

print("å¼€å§‹é€šè¿‡ DataLoader (å•è¿›ç¨‹) éå†...")
start_time_dataloader_single = time.time()
count_dataloader_single = 0
items_dataloader_single = 0
for batch_data, batch_labels in dataloader_single_worker:
    # batch_data å’Œ batch_labels ç°åœ¨æ˜¯ PyTorch Tensors
    # æ¨¡æ‹Ÿæ“ä½œ
    _ = batch_data.shape
    _ = batch_labels.sum() # å¯¹æ‰¹æ¬¡æ ‡ç­¾åšä¸ªç®€å•æ“ä½œ
    count_dataloader_single += 1
    items_dataloader_single += len(batch_data)
end_time_dataloader_single = time.time()
duration_dataloader_single = end_time_dataloader_single - start_time_dataloader_single
print(f"é€šè¿‡ DataLoader (å•è¿›ç¨‹) éå†å®Œæˆã€‚")
print(f"éå†æ‰¹æ¬¡æ•°: {count_dataloader_single}, æ€»æ ·æœ¬æ•°: {items_dataloader_single}")
print(f"è€—æ—¶: {duration_dataloader_single:.4f} ç§’\n")

# 5. é€šè¿‡ DataLoader éå† Dataset (å¤šè¿›ç¨‹)
# æ³¨æ„ï¼šå¤šè¿›ç¨‹åœ¨ Windows ä¸Šæœ‰æ—¶éœ€è¦å°† DataLoader çš„åˆ›å»ºå’Œè¿­ä»£æ”¾åœ¨ if __name__ == '__main__': ä¸­ã€‚
# åœ¨ Jupyter Notebook æˆ–ç±»ä¼¼ç¯å¢ƒä¸­ï¼Œé€šå¸¸å¯ä»¥ç›´æ¥è¿è¡Œã€‚
# num_workers çš„é€‰æ‹©é€šå¸¸å–å†³äºä½ çš„ CPUæ ¸å¿ƒæ•°ã€‚
num_workers_to_test = 4 # ä½ å¯ä»¥æ ¹æ®ä½ çš„ CPU è°ƒæ•´è¿™ä¸ªå€¼
print(f"åˆ›å»º DataLoader å®ä¾‹ (batch_size={batch_size}, num_workers={num_workers_to_test})...")
try:
    dataloader_multi_worker = DataLoader(custom_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers_to_test)
    print(f"DataLoader ({num_workers_to_test}ä¸ªå·¥ä½œè¿›ç¨‹) å®ä¾‹åˆ›å»ºå®Œæ¯•ã€‚\n")

    print(f"å¼€å§‹é€šè¿‡ DataLoader ({num_workers_to_test}ä¸ªå·¥ä½œè¿›ç¨‹) éå†...")
    start_time_dataloader_multi = time.time()
    count_dataloader_multi = 0
    items_dataloader_multi = 0
    for batch_data, batch_labels in dataloader_multi_worker:
        _ = batch_data.shape
        _ = batch_labels.sum()
        count_dataloader_multi += 1
        items_dataloader_multi += len(batch_data)
    end_time_dataloader_multi = time.time()
    duration_dataloader_multi = end_time_dataloader_multi - start_time_dataloader_multi
    print(f"é€šè¿‡ DataLoader ({num_workers_to_test}ä¸ªå·¥ä½œè¿›ç¨‹) éå†å®Œæˆã€‚")
    print(f"éå†æ‰¹æ¬¡æ•°: {count_dataloader_multi}, æ€»æ ·æœ¬æ•°: {items_dataloader_multi}")
    print(f"è€—æ—¶: {duration_dataloader_multi:.4f} ç§’\n")

except RuntimeError as e:
    if " Ï€ÎµÏÎ¹ÏƒÏƒÏŒÏ„Îµprocesses" in str(e) or "BrokenPipeError" in str(e) or "can't start new thread" in str(e) or "unable to start new thread" in str(e):
        print(f"æ³¨æ„: åœ¨æŸäº›ç¯å¢ƒ (ç‰¹åˆ«æ˜¯ Windows æˆ–èµ„æºå—é™çš„ Notebook) ä¸­ç›´æ¥è¿è¡Œå¤šè¿›ç¨‹ DataLoader å¯èƒ½ä¼šé‡åˆ°é—®é¢˜ã€‚")
        print(f"é”™è¯¯ä¿¡æ¯: {e}")
        print("å¦‚æœé‡åˆ°æ­¤é—®é¢˜ï¼Œè¯·å°è¯•å°† DataLoader çš„åˆ›å»ºå’Œè¿­ä»£éƒ¨åˆ†æ”¾åœ¨ `if __name__ == '__main__':` å—ä¸­è¿è¡Œè„šæœ¬ï¼Œæˆ–è€…å‡å°‘ `num_workers`ã€‚")
        duration_dataloader_multi = float('inf') # æ ‡è®°ä¸ºå¤±è´¥æˆ–ä¸å¯ç”¨
    else:
        raise e


# --- æ€»ç»“ ---
print("="*30)
print("å®éªŒç»“æœæ€»ç»“:")
print("="*30)
print(f"ç›´æ¥éå† Dataset ({count_dataset} é¡¹): {duration_dataset:.4f} ç§’")
print(f"é€šè¿‡ DataLoader (num_workers=0, {items_dataloader_single} é¡¹): {duration_dataloader_single:.4f} ç§’")
if duration_dataloader_multi != float('inf'):
    print(f"é€šè¿‡ DataLoader (num_workers={num_workers_to_test}, {items_dataloader_multi} é¡¹): {duration_dataloader_multi:.4f} ç§’")
    if duration_dataloader_single > duration_dataset:
        print(f"\nâ¡ï¸ DataLoader (num_workers=0) æ¯”ç›´æ¥éå† Dataset æ…¢äº†çº¦ {(duration_dataloader_single / duration_dataset - 1) * 100:.2f}%")
    if duration_dataloader_multi > duration_dataset:
        print(f"â¡ï¸ DataLoader (num_workers={num_workers_to_test}) æ¯”ç›´æ¥éå† Dataset æ…¢äº†çº¦ {(duration_dataloader_multi / duration_dataset - 1) * 100:.2f}% (ä½†å¯èƒ½å› ä¸ºå¹¶è¡ŒåŒ–è€Œæ¯” num_workers=0 å¿«)")
    elif duration_dataloader_multi < duration_dataloader_single :
         print(f"\nâœ… DataLoader (num_workers={num_workers_to_test}) æ¯” DataLoader (num_workers=0) å¿«äº†çº¦ {(1 - duration_dataloader_multi / duration_dataloader_single) * 100:.2f}%ï¼Œæ˜¾ç¤ºäº†å¤šè¿›ç¨‹çš„ä¼˜åŠ¿ã€‚")
    if duration_dataloader_multi < duration_dataset:
        print(f"ğŸ”¥ æ³¨æ„ï¼šåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œå¦‚æœæ•°æ®é¢„å¤„ç†éå¸¸ç®€å•ä¸” num_workers åˆç†ï¼Œå¤šè¿›ç¨‹ DataLoader ç”šè‡³å¯èƒ½æ¯”ç›´æ¥ä¸²è¡Œéå† Dataset æ›´å¿«ï¼Œä½†è¿™ä¸å¸¸è§äºçº¯è¿­ä»£ NumPy æ•°ç»„çš„åœºæ™¯ã€‚")

else:
    print(f"å¤šè¿›ç¨‹ DataLoader (num_workers={num_workers_to_test}) æµ‹è¯•æœªèƒ½æˆåŠŸå®Œæˆã€‚")
    if duration_dataloader_single > duration_dataset:
        print(f"\nâ¡ï¸ DataLoader (num_workers=0) æ¯”ç›´æ¥éå† Dataset æ…¢äº†çº¦ {(duration_dataloader_single / duration_dataset - 1) * 100:.2f}%")

print("\nå®éªŒåˆ†æ:")
print("1. **ç›´æ¥éå† Dataset** é€šå¸¸æ˜¯æœ€å¿«çš„ï¼Œå› ä¸ºå®ƒåªæ¶‰åŠåˆ° Python çš„è¿­ä»£å™¨åè®®å’Œ NumPy æ•°ç»„çš„ç›´æ¥è®¿é—®ã€‚")
print("2. **DataLoader (num_workers=0)** ä¼šå¼•å…¥é¢å¤–çš„å¼€é”€ï¼Œå› ä¸ºæ•°æ®éœ€è¦ç»è¿‡ `collate_fn` (å³ä½¿æ˜¯é»˜è®¤çš„) è¿›è¡Œæ‰¹å¤„ç†ï¼Œå¹¶ä¸”ä» NumPy æ•°ç»„è½¬æ¢ä¸º PyTorch Tensor (å¦‚æœ `__getitem__` è¿”å›çš„æ˜¯ NumPy æ•°ç»„)ã€‚è¿™äº›æ“ä½œåœ¨ä¸»è¿›ç¨‹ä¸­ä¸²è¡Œæ‰§è¡Œã€‚")
print("3. **DataLoader (num_workers > 0)** ä¼šä½¿ç”¨å¤šä¸ªå­è¿›ç¨‹æ¥å¹¶è¡ŒåŠ è½½å’Œé¢„å¤„ç†æ•°æ®ã€‚è™½ç„¶è¿™å¯ä»¥æ˜¾è‘—åŠ é€Ÿæ•´ä¸ªæ•°æ®åŠ è½½æµç¨‹ï¼ˆç‰¹åˆ«æ˜¯å½“ `__getitem__` ä¸­çš„é¢„å¤„ç†æ¯”è¾ƒè€—æ—¶æ—¶ï¼‰ï¼Œä½†è¿›ç¨‹çš„åˆ›å»ºã€ç®¡ç†ã€æ•°æ®åœ¨è¿›ç¨‹é—´çš„ä¼ é€’ï¼ˆåºåˆ—åŒ–å’Œååºåˆ—åŒ–ï¼‰æœ¬èº«ä¹Ÿä¼šå¼•å…¥å¼€é”€ã€‚å¯¹äºä»å†…å­˜ä¸­çš„ NumPy æ•°ç»„è¯»å–è¿™æ ·IOå¼€é”€æå°ã€CPUè®¡ç®—ä¹Ÿæå°‘çš„æƒ…å†µï¼Œå¤šè¿›ç¨‹çš„å¼€é”€å¯èƒ½ä¼šè¶…è¿‡å…¶å¸¦æ¥çš„å¹¶è¡Œä¼˜åŠ¿ï¼Œå¯¼è‡´å®ƒæ¯” `num_workers=0` ç”šè‡³ç›´æ¥éå† `Dataset` æ›´æ…¢ã€‚")
print("   - ç„¶è€Œï¼Œå¦‚æœ `__getitem__` ä¸­åŒ…å«å¤æ‚çš„è½¬æ¢æˆ–ä»ç£ç›˜è¯»å–æ•°æ®ï¼Œ`num_workers > 0` é€šå¸¸ä¼šå¸¦æ¥æ˜¾è‘—çš„æ€§èƒ½æå‡ã€‚")
print("   - `DataLoader` çš„ä¸»è¦ä¼˜åŠ¿åœ¨äºå®ƒèƒ½å¤Ÿéšè—æ•°æ®åŠ è½½çš„å»¶è¿Ÿï¼Œä½¿å¾— GPU åœ¨è®­ç»ƒæ—¶ä¸ä¼šå› ä¸ºç­‰å¾…æ•°æ®è€Œç©ºé—²ã€‚")